{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd1d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "# home\n",
    "#hostip = \"192.168.1.101\" \n",
    "\n",
    "\n",
    "# library\n",
    "hostip = '10.192.0.143'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2260ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, element_at, when\n",
    "import pygeohash as pgh\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "\n",
    "#Initialize our spark session with \n",
    "#threads = #logicalCPU and the given application name.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e47a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming dataframe with options \n",
    "# providing the bootstrap server(s) and topic name.\n",
    "\n",
    "topic_stream_df1 = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "    .option('subscribe', 'PartB1')\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "union_stream = topic_stream_df1\n",
    "\n",
    "stream = union_stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04e01c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_data(batch_df, batch_id):\n",
    "    \n",
    "    # print(f\"Processing batch: {batch_id}\")\n",
    "    collected_data = batch_df.collect()\n",
    "    \n",
    "    # this climate will go to the inside to database\n",
    "    climate = {}\n",
    "    \n",
    "    # initial list for calculating stage\n",
    "    initial_AQUA = []\n",
    "    initial_TERA = []\n",
    "\n",
    "    try:\n",
    "        if len(collected_data) > 0:\n",
    "            \n",
    "            \n",
    "            for raw in collected_data:\n",
    "                \n",
    "                # make json format\n",
    "                data = raw.asDict()\n",
    "                data = data['value']\n",
    "                data = json.loads(data)\n",
    "                \n",
    "                \n",
    "                # data pre processing based on producer\n",
    "                if data['producer'] == 'producer1':\n",
    "                    data['latitude'] = float(data['latitude'])\n",
    "                    data['longitude'] = float(data['longitude'])\n",
    "                    data['air_temperature_celcius'] = float(data['air_temperature_celcius'])\n",
    "                    data['relative_humidity'] = float(data['relative_humidity'])\n",
    "                    data['windspeed_knots'] = float(data['windspeed_knots'])\n",
    "                    data['max_wind_speed'] = float(data['max_wind_speed'])\n",
    "                    data['GHI_w/m2'] = float(data['GHI_w/m2'])                                \n",
    "                    created_time_obj = datetime.datetime.strptime(data['created_time'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "                    formatted_time = created_time_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    data['created_time'] = formatted_time\n",
    "                    \n",
    "                    climate = data\n",
    "                    \n",
    "                elif data['producer'] == 'producer2':\n",
    "                    data['latitude'] = float(data['latitude'])\n",
    "                    data['longitude'] = float(data['longitude'])\n",
    "                    data['confidence'] = float(data['confidence'])\n",
    "                    data['surface_temperature_celcius'] = float(data['surface_temperature_celcius'])\n",
    "                    created_time_obj = datetime.datetime.strptime(data['created_time'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "                    formatted_time = created_time_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    data['created_time'] = formatted_time\n",
    "                    initial_AQUA.append(data)\n",
    "                    \n",
    "                else:\n",
    "                    data['latitude'] = float(data['latitude'])\n",
    "                    data['longitude'] = float(data['longitude'])\n",
    "                    data['confidence'] = float(data['confidence'])\n",
    "                    data['surface_temperature_celcius'] = float(data['surface_temperature_celcius'])\n",
    "                    created_time_obj = datetime.datetime.strptime(data['created_time'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "                    formatted_time = created_time_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    data['created_time'] = formatted_time\n",
    "                    initial_TERA.append(data)\n",
    "                \n",
    "                # second stage : geohash (compare climate and initial datas)       \n",
    "                temp_hotspot = []   \n",
    "\n",
    "                # Process climate data\n",
    "                if climate != {} :\n",
    "\n",
    "                    cli_long = climate['longitude']\n",
    "                    cli_lat = climate['latitude']\n",
    "            \n",
    "                    # pygeohash format\n",
    "                    cli_encode_info = pgh.encode(latitude=cli_lat, longitude=cli_long, precision=3)\n",
    "\n",
    "                    \n",
    "                    # now, need to think about the AQUA initial and TERRA initial\n",
    "                    \n",
    "                    for A_record in initial_AQUA:\n",
    "                        AQUA_encode_info = pgh.encode(latitude = A_record['latitude'], longitude = A_record['longitude'], precision = 3 )\n",
    "                        \n",
    "                        if AQUA_encode_info == cli_encode_info:\n",
    "                            A_record['sate'] = 'AQUA'\n",
    "                            temp_hotspot.append(A_record)\n",
    "                            \n",
    "                    for T_record in initial_TERA:\n",
    "                        TERA_encode_info = pgh.encode(latitude = T_record['latitude'], longitude = T_record['longitude'], precision = 3 )\n",
    "                        \n",
    "                        if TERA_encode_info == cli_encode_info:\n",
    "                            T_record['sate'] = 'TERRA'\n",
    "                            temp_hotspot.append(T_record)\n",
    "                            \n",
    "                    # Merge ‘surface temperature’ and ‘confidence’\n",
    "                    \n",
    "                    if len(temp_hotspot) > 1:\n",
    "                        \n",
    "                        # checking enter here\n",
    "                        # print(\"alright, need to merge ~ \")\n",
    "                        \n",
    "                        merged_hotspot = list()\n",
    "                        \n",
    "                        merge_AQUA = list()\n",
    "                        merge_TERA = list()\n",
    "                        \n",
    "                        for hotspot_info in temp_hotspot:\n",
    "                            if hotspot_info['sate'] == 'AQUA':\n",
    "                                merge_AQUA.append(hotspot_info)\n",
    "                            else:\n",
    "                                merge_TERA.append(hotspot_info)\n",
    "                                \n",
    "                        if len(merge_AQUA) > 0 and len(merge_TERA) > 0:\n",
    "                            \n",
    "                            for i in merge_AQUA:\n",
    "                                compare5_AQUA_lat = i['latitude']\n",
    "                                compare5_AQUA_long = i['longitude']\n",
    "                                \n",
    "                                for j in merge_TERA:\n",
    "                                    encode5_AQUA = pgh.encode(latitude = compare5_AQUA_lat, longitude = compare5_AQUA_long, precision = 5)\n",
    "                                    encode5_TERA = pgh.encode(latitude = j['latitude'], longitude = j['longitude'], precision = 5)\n",
    "                                    \n",
    "                                    # final check\n",
    "                                    if encode5_AQUA == encode5_TERA:\n",
    "                                        legit = dict()\n",
    "                                        \n",
    "                                        legit['avg_temperature'] = (i['surface_temperature_celcius'] + j['surface_temperature_celcius']) / 2\n",
    "                                        legit['avg_confidence'] = (i['confidence'] + j['confidence']) / 2\n",
    "                                        legit['latitude']  = compare5_AQUA_lat\n",
    "                                        legit['longitude'] = compare5_AQUA_long\n",
    "                                        \n",
    "                                        merged_hotspot.append(legit)\n",
    "                    \n",
    "\n",
    "                    # checking is the event is natural or event\n",
    "                    if len(temp_hotspot) > 0:\n",
    "                        air_temp = float(climate['air_temperature_celcius'])\n",
    "                        GHI = float(climate['GHI_w/m2'])\n",
    "                        event = 'other'\n",
    "\n",
    "                        if air_temp > 20 and GHI > 180:\n",
    "                            event = 'natural'\n",
    "\n",
    "                        for h in temp_hotspot:\n",
    "                            h['event'] = event\n",
    "                    \n",
    "                    climate['hotspot'] = temp_hotspot\n",
    "            \n",
    "                    try:\n",
    "                        db.hotspot.insert_one(climate)\n",
    "                        \n",
    "                    except pymongo.errors.DuplicateKeyError:\n",
    "                        # This key has already in the database, so shouldn't use it.\n",
    "                        pass\n",
    "                    \n",
    "                    except Exception as ex:\n",
    "                        \n",
    "                        print(\"An error occurred:\", ex)\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a48a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_writer = (\n",
    "    stream\n",
    "    .writeStream\n",
    "    .foreachBatch(process_data)\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime='10 seconds')\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b06852fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# home\n",
    "# client = MongoClient('mongodb://192.168.1.101:27017/')\n",
    "\n",
    "# library\n",
    "client = MongoClient('mongodb://10.192.0.143:27017/')\n",
    "\n",
    "\n",
    "# list of database before we delete\n",
    "result = client.list_database_names()\n",
    "#print(result)\n",
    "\n",
    "# make database\n",
    "db = client.fit3182_assignment1_db\n",
    "\n",
    "# add new collection into new database\n",
    "db.hotspot.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7946f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def stop_db_writer():\n",
    "    print('Stopping query after timeout.')\n",
    "    db_writer.stop()\n",
    "\n",
    "try:\n",
    "    # it will automatically stop with  stop_db_writer fuction, 5 min\n",
    "    timer = threading.Timer(30, stop_db_writer)\n",
    "    timer.start()\n",
    "    \n",
    "    # wait till it finish\n",
    "    db_writer.awaitTermination()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "    db_writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02652cee",
   "metadata": {},
   "source": [
    "There will be an error if the key (_id) has been collusion, therefore, i used try and except( Duplicate Key Error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.hotspot.find({})\n",
    "for document in cursor:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd7b1e",
   "metadata": {},
   "source": [
    "The below function which is \"process_data2\" is the fucntion for in case if merging is not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926102d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
